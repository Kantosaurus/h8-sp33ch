{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d338d056",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02cdbaaf",
   "metadata": {},
   "source": [
    "# Task 2: Apply Dimension Reduction Techniques (10 marks)\n",
    "# \n",
    "#### **Objective:** Apply Principal Component Analysis (PCA) to reduce the dimensionality of TF-IDF features from 5000 dimensions to various lower dimensions, then evaluate performance using K-Nearest Neighbors (KNN) classifier.\n",
    "# \n",
    "\n",
    "#### **Key Task Deliverables:**\n",
    "#### - **2a.** Code implementation of PCA on train and test sets (sklearn package allowed)\n",
    "#### - **2b.** Report Macro F1 scores for 2000, 1000, 500, and 100 components using KNN (n_neighbors=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a60ce29",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "537c92d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996faee5",
   "metadata": {},
   "source": [
    "### Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f715f31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data():\n",
    "    \"\"\"\n",
    "    Load and prepare the training and test data\n",
    "    \"\"\"\n",
    "    print(\"Loading training data...\")\n",
    "    train_data = pd.read_csv('data/train_tfidf_features.csv')\n",
    "    \n",
    "    print(\"Loading test data...\")\n",
    "    test_data = pd.read_csv('data/test_tfidf_features.csv')\n",
    "    \n",
    "    print(f\"Training data shape: {train_data.shape}\")\n",
    "    print(f\"Test data shape: {test_data.shape}\")\n",
    "    \n",
    "    # Separate features and labels\n",
    "    X_train = train_data.drop(['id', 'label'], axis=1).values\n",
    "    y_train = train_data['label'].values\n",
    "    \n",
    "    X_test = test_data.drop(['id'], axis=1).values\n",
    "    \n",
    "    print(f\"X_train shape: {X_train.shape}\")\n",
    "    print(f\"y_train shape: {y_train.shape}\")\n",
    "    print(f\"X_test shape: {X_test.shape}\")\n",
    "    \n",
    "    # Check for any missing values\n",
    "    print(f\"Missing values in X_train: {np.isnan(X_train).sum()}\")\n",
    "    print(f\"Missing values in y_train: {np.isnan(y_train).sum()}\")\n",
    "    print(f\"Missing values in X_test: {np.isnan(X_test).sum()}\")\n",
    "    \n",
    "    # Handle any missing values\n",
    "    X_train = np.nan_to_num(X_train, nan=0.0)\n",
    "    X_test = np.nan_to_num(X_test, nan=0.0)\n",
    "    \n",
    "    return X_train, y_train, X_test, train_data['id'].values, test_data['id'].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f34460e",
   "metadata": {},
   "source": [
    "### Load and prepare data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69d0f934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Task 2: Dimension Reduction using PCA ===\n",
      "Loading and preparing data...\n",
      "Loading training data...\n",
      "Loading test data...\n",
      "Training data shape: (17184, 5002)\n",
      "Test data shape: (4296, 5001)\n",
      "X_train shape: (17184, 5000)\n",
      "y_train shape: (17184,)\n",
      "X_test shape: (4296, 5000)\n",
      "Missing values in X_train: 0\n",
      "Missing values in y_train: 0\n",
      "Missing values in X_test: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Task 2: Dimension Reduction using PCA ===\")\n",
    "print(\"Loading and preparing data...\")\n",
    "X_train, y_train, X_test, train_ids, test_ids = load_and_prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759d0a12",
   "metadata": {},
   "source": [
    "### Display class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75d6e09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class distribution in training data:\n",
      "Class 0: 10633 samples (61.88%)\n",
      "Class 1: 6551 samples (38.12%)\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nClass distribution in training data:\")\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "for label, count in zip(unique, counts):\n",
    "    print(f\"Class {label}: {count} samples ({count/len(y_train)*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008775c2",
   "metadata": {},
   "source": [
    "### Task 2a: Code Implementation of PCA on Train and Test Sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8e0bdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pca_and_evaluate(X_train, y_train, X_test, n_components_list):\n",
    "    \"\"\"\n",
    "    Apply PCA with different numbers of components and evaluate using KNN\n",
    "    \n",
    "    Task 2a: PCA implementation on train and test sets using sklearn\n",
    "    Task 2b: KNN training with n_neighbors=2 for Macro F1 evaluation\n",
    "    \"\"\"\n",
    "    # Create validation split for F1 score evaluation\n",
    "    X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "        X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    "    )\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for n_components in n_components_list:\n",
    "        print(f\"\\n=== PCA with {n_components} components ===\")\n",
    "        \n",
    "        # Task 2a: Apply PCA using sklearn\n",
    "        print(\"Applying PCA transformation...\")\n",
    "        pca = PCA(n_components=n_components, random_state=42)\n",
    "        X_train_pca = pca.fit_transform(X_train_split)\n",
    "        X_val_pca = pca.transform(X_val_split)\n",
    "        X_test_pca = pca.transform(X_test)\n",
    "        \n",
    "        print(f\"Original feature space: {X_train.shape[1]} dimensions\")\n",
    "        print(f\"Reduced feature space: {X_train_pca.shape[1]} dimensions\")\n",
    "        print(f\"Explained variance ratio: {pca.explained_variance_ratio_.sum():.4f}\")\n",
    "        \n",
    "        # Task 2b: Train KNN model with n_neighbors=2\n",
    "        print(\"Training KNN classifier (n_neighbors=2)...\")\n",
    "        knn = KNeighborsClassifier(n_neighbors=2)\n",
    "        knn.fit(X_train_pca, y_train_split)\n",
    "        \n",
    "        # Make predictions on validation set for F1 score evaluation\n",
    "        y_pred_val = knn.predict(X_val_pca)\n",
    "        \n",
    "        # Calculate validation metrics\n",
    "        f1_macro = f1_score(y_val_split, y_pred_val, average='macro')\n",
    "        accuracy = accuracy_score(y_val_split, y_pred_val)\n",
    "        \n",
    "        print(f\"PCA with {n_components} components: Accuracy = {accuracy}, F1 Score = {f1_macro}\")\n",
    "        \n",
    "        # Make predictions on test set for Kaggle submission\n",
    "        y_pred = knn.predict(X_test_pca)\n",
    "        \n",
    "        # Store results\n",
    "        results[n_components] = {\n",
    "            'pca': pca,\n",
    "            'knn': knn,\n",
    "            'X_train_pca': X_train_pca,\n",
    "            'X_test_pca': X_test_pca,\n",
    "            'predictions': y_pred,\n",
    "            'explained_variance_ratio': pca.explained_variance_ratio_.sum()\n",
    "        }\n",
    "        \n",
    "        print(f\"PCA with {n_components} components completed\")\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af612157",
   "metadata": {},
   "source": [
    "### Task 2b: Define components to test as per requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f408417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Applying PCA with different numbers of components ===\n",
      "Testing with components: [2000, 1000, 500, 100]\n",
      "\n",
      "=== PCA with 2000 components ===\n",
      "Applying PCA transformation...\n",
      "Original feature space: 5000 dimensions\n",
      "Reduced feature space: 2000 dimensions\n",
      "Explained variance ratio: 0.8345\n",
      "Training KNN classifier (n_neighbors=2)...\n",
      "PCA with 2000 components: Accuracy = 0.49869071864998543, F1 Score = 0.49788123970603515\n",
      "PCA with 2000 components completed\n",
      "\n",
      "=== PCA with 1000 components ===\n",
      "Applying PCA transformation...\n",
      "Original feature space: 5000 dimensions\n",
      "Reduced feature space: 1000 dimensions\n",
      "Explained variance ratio: 0.6544\n",
      "Training KNN classifier (n_neighbors=2)...\n",
      "PCA with 1000 components: Accuracy = 0.5987780040733197, F1 Score = 0.5583393880128434\n",
      "PCA with 1000 components completed\n",
      "\n",
      "=== PCA with 500 components ===\n",
      "Applying PCA transformation...\n",
      "Original feature space: 5000 dimensions\n",
      "Reduced feature space: 500 dimensions\n",
      "Explained variance ratio: 0.4866\n",
      "Training KNN classifier (n_neighbors=2)...\n",
      "PCA with 500 components: Accuracy = 0.6418388129182426, F1 Score = 0.550399985377987\n",
      "PCA with 500 components completed\n",
      "\n",
      "=== PCA with 100 components ===\n",
      "Applying PCA transformation...\n",
      "Original feature space: 5000 dimensions\n",
      "Reduced feature space: 100 dimensions\n",
      "Explained variance ratio: 0.2092\n",
      "Training KNN classifier (n_neighbors=2)...\n",
      "PCA with 100 components: Accuracy = 0.642129764329357, F1 Score = 0.5575122776314743\n",
      "PCA with 100 components completed\n"
     ]
    }
   ],
   "source": [
    "n_components_list = [2000, 1000, 500, 100]\n",
    "\n",
    "print(f\"\\n=== Applying PCA with different numbers of components ===\")\n",
    "print(f\"Testing with components: {n_components_list}\")\n",
    "\n",
    "results = apply_pca_and_evaluate(X_train, y_train, X_test, n_components_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e55939f",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20f6e408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Results Summary ===\n",
      "Number of Components | Explained Variance Ratio\n",
      "---------------------------------------------\n",
      "       2000         |         0.8345         \n",
      "       1000         |         0.6544         \n",
      "        500         |         0.4866         \n",
      "        100         |         0.2092         \n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n=== Results Summary ===\")\n",
    "print(\"Number of Components | Explained Variance Ratio\")\n",
    "print(\"-\" * 45)\n",
    "for n_components in n_components_list:\n",
    "    var_ratio = results[n_components]['explained_variance_ratio']\n",
    "    print(f\"{n_components:^19} | {var_ratio:^23.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c4c50a",
   "metadata": {},
   "source": [
    "#### First 10 Results from each .csv file\n",
    "\n",
    "| ID | PCA 2000 Components | PCA 1000 Components | PCA 500 Components | PCA 100 Components |\n",
    "|----|--------------------|--------------------|--------------------|-------------------|\n",
    "| 17185 | 1 | 1 | 0 | 1 |\n",
    "| 17186 | 1 | 0 | 0 | 0 |\n",
    "| 17187 | 1 | 1 | 0 | 0 |\n",
    "| 17188 | 1 | 0 | 0 | 0 |\n",
    "| 17189 | 0 | 0 | 0 | 0 |\n",
    "| 17190 | 1 | 1 | 1 | 1 |\n",
    "| 17191 | 1 | 0 | 0 | 0 |\n",
    "| 17192 | 0 | 0 | 0 | 0 |\n",
    "| 17193 | 0 | 0 | 0 | 0 |\n",
    "| 17194 | 1 | 1 | 0 | 1 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e7aa09",
   "metadata": {},
   "source": [
    "### Complete Results Table:\n",
    "\n",
    "| Number of Components | Explained Variance Ratio | Local Validation F1 Score | **Macro F1 Score (Kaggle)** |\n",
    "|---------------------|-------------------------|---------------------|-------------------------------------|\n",
    "| 2000                | 0.8345                  | 0.49788            | **0.38337**                         |\n",
    "| 1000                | 0.6544                  | 0.55834            | **0.72043**                         |\n",
    "| 500                 | 0.4866                  | 0.55040            | **0.84799**                         |\n",
    "| 100                 | 0.2092                  | 0.55751            | **0.81797**                         |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a2aa96",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
